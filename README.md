# Homography-based loss function for camera pose regression
In this repository, we share our implementation of several camera pose regression
loss functions in a simple end-to-end network similar to
[PoseNet](https://openaccess.thecvf.com/content_iccv_2015/html/Kendall_PoseNet_A_Convolutional_ICCV_2015_paper.html).
We implemented our homography-based loss functions and re-implemented PoseNet, Homoscedastic, Geometric and DSAC loss
functions. We provide the code to train the network and evaluate their performance on the Cambridge dataset.

## Installation

### Dataset setup
Have a look at the [datasets](datasets) folder to setup the Cambridge dataset.

### Python environment setup
We use `python 3.9.7` and `pip 21.2.4`. Modules requirements are listed in [requirements.txt](requirements.txt).
An easy way to setup the python environment is to have [Anaconda](https://www.anaconda.com) installed.  
Setting up an anaconda environment:
```bash
conda create -n homographyloss python=3.9.7 pip=21.2.4
conda activate homographyloss
pip install -r requirements.txt
```

## Run relocalization
The script [main.py](main.py) trains the network on a given scene.
It requires one positional argument: the path to the scene on which to train the model.
For example, for training the model on ShopFacade, simply run:
```bash
python main.py datasets/ShopFacade
```

Other available training options can be listed by running `python main.py -h`.

## Monitor training and test results
Training and test metrics are saved in a `logs` directory. One can monitor them using tensorboard.
Simply run in a new terminal:
```bash
tensorboard --logdir logs
```

All estimated poses are also saved in a CSV file.

## Getting the new dataset
By the new dataset, we mean the original dataset in addition to the corresponding depth image for each of the images in the original set. 
There are two ways to do so:

1- Generating the depth images from the depth estimation network and then merge them with the original dataset

2- Downloading the modified dataset from google drive directly by simple running these commands:
```bash
gdown https://drive.google.com/uc?id=1Nnz9SNve7ZPKnextdALXDJACrlNU8Bio

unzip ShopFacade_depth.zip

rm ShopFacade_depth.zip
``` 
or perhaps put them in a bash script and then put ``` #!/bin/bash ``` in the first line so that you can run it in more automatic way.                   

## Using the Depth Estimator Network (DEN) for x_min and x_max
### Train
To train the current model with DEN, use the job.bash file. The weights for the trained model can be downloaded from this [link](https://drive.google.com/file/d/1WO_43540q_9Sc0GceSzCmqkK6WW-n9F1/view?usp=sharing). 

### Test
To test, run the following command
```
python main_depth_test.py datasets/ShopFacade --cuda --weights weights.pth
```

### Analysis
The training and testing results have been pushed to tensor board website and can be accessed by clicking [here](https://tensorboard.dev/experiment/lTJVKgIEROO52afv0ilpRQ/#scalars)

A detailed analysis have been performed and the results can be visualised with the help of [google colab](https://drive.google.com/file/d/1Pn0GEs-xWpFGSOI9EEhGlxpXMwnMnc7c/view?usp=sharing)

## Modified models to take RGB-D input instead of RGB only
We changed the the input layer of the model by adding another channel in the first conv2d layer. Now the model will take 3 channels of the image in addition to the corresponding depth image generated by the depth estimation network. We tried this approach with slight modofications to each one.

1-depth_non_freeze_local_homography.py : is to run the modified model and in the same time updating all the pre-trained weights during the training. The loss function uesd here was the local homography

2-depth_non_freeze_global_homography.py : is to run the modified model and in the same time updating all the pre-trained weights during the training. The loss function uesd here was the global homography

3-depth_non_freeze_local_homography_add_1layer.py : is to run the modified model and in the same time updating all the pre-trained weights during the training. We added a new conv2d layer after the input layer to make the model more complex and able to learn more. The loss function uesd here was the local homography

4-depth_non_freeze_global_homography_add_1layer.py : is to run the modified model and in the same time updating all the pre-trained weights during the training. We added a new conv2d layer after the input layer to make the model more complex and able to learn more. The loss function uesd here was the global homography

5-depth_all_freeze_local_homography.py : is to run the modified model but with freezing all the pre-trained weights during the training except for the weights of the modified input layer. The loss function uesd here was the local homography

6-depth_all_freeze_global_homography.py : is to run the modified model but with freezing all the pre-trained weights during the training except for the weights of the modified input layer. The loss function uesd here was the global homography

7-original_local_homography.py: is to run the original model just to see its performance and compare it with the results of our modified models. The loss function used is local homography 

8-original_global_homography.py: is to run the original model just to see its performance and compare it with the results of our modified models. The loss function used is global homography 

We trained all of the above models for 500 epochs. The obtained events files are saved in all_logs folder which can be used by tensorboard to visalize and analyze the results by simply navigating to all_logs folder and then write: tensorboard --logdir <name of the log folder>
 
We also created bash files to be sent to the cluster to run these models. These bash scipts can be found in jobs folder 

Thank you
